{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99b825d0",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"imgs/7-23.png\" width=600></p>\n",
    "\n",
    "위의 계층들을 순서대로 코드로 구현해보면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81387d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.util import im2col, col2im\n",
    "import numpy as np\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        FN, C, FH, FW = self.W.shape # 필터개수, 채널, 필터높이, 필터너비\n",
    "        N, C, H, W = self.x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH)/self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW)/self.stride)\n",
    "        print('out_h:', out_h)\n",
    "        print('out_w:', out_w)\n",
    "\n",
    "        self.col = im2col(self.x, FH, FW, self.stride, self.pad)\n",
    "        print('col shape:', self.col.shape)\n",
    "        self.col_W = self.W.reshape(FN, -1).T # 필터전개\n",
    "        print('self.col_W shape:', self.col_W.shape)\n",
    "        out = np.dot(self.col, self.col_W) + self.b\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout): # 책에서는 설명하지 않음\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "982b821d",
   "metadata": {},
   "source": [
    "convoluiton을 하나씩 뜯어보기 위해, 이미지를 아무거나 하나 넣어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eedcea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 300, 300)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "x = cv2.imread('imgs/zero.png')\n",
    "x = cv2.resize(x, (300, 300))\n",
    "\n",
    "print(x.shape)\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "\n",
    "x = x.transpose(2, 0, 1)\n",
    "x = np.expand_dims(x, axis=0) # 차원 수를 늘려줌 (convolution에 넣기 위해)\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc763f10",
   "metadata": {},
   "source": [
    "* opencv에서 이미지를 읽어오면 (H, W, C) 순으로 들어가는데, 책에서는 (C, H, W) 순으로 설명한다.\n",
    "* 배치 처리를 하므로, 항상 shape의 맨 앞은 N (이미지 개수)\n",
    "* 왜 W + b 모양이 다른데 연산이 가능할까? -> 실제로는 bias가 broadcast 되기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f262fbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_h: 291\n",
      "out_w: 291\n",
      "col shape: (84681, 300)\n",
      "col_W shape: (300, 1)\n",
      "84681\n",
      "out shape: (1, 3, 291, 291)\n"
     ]
    }
   ],
   "source": [
    "W = np.array([2]*300).reshape(1, 3, 10, 10) # N, C, FH, FW\n",
    "b = np.array([1]*3).reshape(3, 1, 1)\n",
    "\n",
    "conv = Convolution(W, b)\n",
    "out = conv.forward(x)\n",
    "print(1 * 291 * 291) # out_h x out_w\n",
    "print('out shape:', out.shape) # 곱셈이 왜 동작하는지 shape를 중점적으로 살펴볼 것"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a70f68d",
   "metadata": {},
   "source": [
    "im2col이 정확히 어떤 원리로 작동하는지는 모르지만.. 몇 가지 알게된 사실이 있다.\n",
    "\n",
    "* im2col의 목적은 행렬의 곱셈을 위한 것이다. (곱셈을 편하게 하려고 펼쳐주는 역할)\n",
    "* 각각의 원소들은 모두 input으로부터 오는 것이다. (별도의 곱셈, 다른 행렬 개입 안 함)\n",
    "* output shape를 살펴보면, 첫 번째 차원은 output_shape와 관련이 있고, <br> 두번째 차원은 weight의 차원과 관련이 있다. (여기서 weight는 kernel/filter와 동일 의미로 쓰임)\n",
    "\n",
    "정리: convolution의 forward를 거치게 되면, 이미지와 필터의 연산 결과가 나온다. (패딩까지 거친 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c263d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from common.layers import Relu, Pooling, Affine, SoftmaxWithLoss\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim = (1, 28, 28),\n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5,\n",
    "                             'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1] # 28\n",
    "        \n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # initialize weight and bias -----------------------------------------------\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(\n",
    "                                                filter_num, input_dim[0],\n",
    "                                                filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(\n",
    "                                                pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # define layers-------------------------------------------------------------\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for k, layer in self.layers.items():\n",
    "            \n",
    "            x = layer.forward(x)\n",
    "            print(k, x.shape)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y ,t)\n",
    "    \n",
    "\n",
    "    def gradient(self, x, t):\n",
    "    \n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bcfe584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 28, 28)\n",
      "out_h: 24\n",
      "out_w: 24\n",
      "col shape: (576, 25)\n",
      "col_W shape: (25, 30)\n",
      "Conv1 (1, 30, 24, 24)\n",
      "Relu1 (1, 30, 24, 24)\n",
      "Pool1 (1, 30, 12, 12)\n",
      "Affine1 (1, 100)\n",
      "Relu2 (1, 100)\n",
      "Affine2 (1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.09180698,  0.09401261, -0.02633058,  0.01313055, -0.01488398,\n",
       "         0.3512742 ,  0.74371533, -0.26237844, -0.14738869, -0.30274706]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = cv2.imread('imgs/zero.png', 0)\n",
    "x = cv2.resize(x, (28, 28))\n",
    "\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print(x.shape)\n",
    "\n",
    "conv = SimpleConvNet()\n",
    "conv.predict(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8657674f",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"imgs/7-23-2.png\" width=800></p>\n",
    "\n",
    "* 여기 그림에서는 행렬곱이 아니라, 컨볼루션 연산이라는 점 상기\n",
    "* 렐루 함수를 거치면? -> element-wise 연산이다.\n",
    "* pooling을 거치면 크기가 반으로 줄어드는 이유? -> pooling_stride = 2 라서\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0d49034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_h: 24\n",
      "out_w: 24\n",
      "col shape: (576, 25)\n",
      "col_W shape: (25, 30)\n",
      "Conv1 (1, 30, 24, 24)\n",
      "Relu1 (1, 30, 24, 24)\n",
      "Pool1 (1, 30, 12, 12)\n",
      "Affine1 (1, 100)\n",
      "Relu2 (1, 100)\n",
      "Affine2 (1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.26488818,  0.01174983, -0.47830995,  0.04593951, -0.72914841,\n",
       "        -0.37771665,  0.36843294, -0.06644674,  0.21642373, -0.35672596]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = SimpleConvNet()\n",
    "conv.predict(x) # 스크롤 올리기 귀찮아서 한번 더"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a6d4890",
   "metadata": {},
   "source": [
    "### Affine1 계층 설명\n",
    "\n",
    "<p align=\"center\"><img src=\"imgs/7-23-3.png\" width=800></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7876d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4320\n",
      "W2 shape: (4320, 100)\n",
      "b2 shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Pool1 (1, 30, 12, 12) <- 위에 보면 된다\n",
    "print(12 * 12 * 30)\n",
    "print('W2 shape:', conv.params['W2'].shape)\n",
    "print('b2 shape:', conv.params['b2'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b5310e0",
   "metadata": {},
   "source": [
    "### Affine2 계층 설명\n",
    "\n",
    "<p align=\"center\"><img src=\"imgs/7-23-4.png\" width=600></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db1b3d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W3 shape: (100, 10)\n",
      "b3 shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "print('W3 shape:', conv.params['W3'].shape)\n",
    "print('b3 shape:', conv.params['b3'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b6b753f",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15d4fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.optimizer import *\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimzer\n",
    "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
    "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.network.params, grads)\n",
    "        \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4cb700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300177861899461\n",
      "=== epoch:1, train acc:0.229, test acc:0.223 ===\n",
      "train loss:2.2980584458726114\n",
      "train loss:2.293705286261003\n",
      "train loss:2.2876166868191103\n",
      "train loss:2.284209240423866\n",
      "train loss:2.2715181203737935\n",
      "train loss:2.262268965544775\n",
      "train loss:2.246694304104042\n",
      "train loss:2.2395524836144727\n",
      "train loss:2.16842030127059\n",
      "train loss:2.181955900235561\n",
      "train loss:2.15036487053864\n",
      "train loss:2.121709996542082\n",
      "train loss:2.068778124544357\n",
      "train loss:2.02429084000044\n",
      "train loss:1.9853480420786083\n",
      "train loss:1.9145568912345694\n",
      "train loss:1.8592888567144912\n",
      "train loss:1.788436466990922\n",
      "train loss:1.7344000075947954\n",
      "train loss:1.6955771384291543\n",
      "train loss:1.513899438447208\n",
      "train loss:1.3972535799101429\n",
      "train loss:1.367995221219077\n",
      "train loss:1.3279235355277204\n",
      "train loss:1.1682030662890002\n",
      "train loss:1.167122824612464\n",
      "train loss:1.182796805661198\n",
      "train loss:0.9172824328484671\n",
      "train loss:0.9725829480862646\n",
      "train loss:1.0172312757666986\n",
      "train loss:0.9567129805388145\n",
      "train loss:0.9415521927996149\n",
      "train loss:0.7863248846813495\n",
      "train loss:0.9097919803184606\n",
      "train loss:0.8566369890348231\n",
      "train loss:0.6375706912758231\n",
      "train loss:0.7296999910023498\n",
      "train loss:0.7085451203257691\n",
      "train loss:0.7254417519176584\n",
      "train loss:0.6800999835083119\n",
      "train loss:0.63914490712423\n",
      "train loss:0.7287851631431178\n",
      "train loss:0.5668945028871585\n",
      "train loss:0.823384740879931\n",
      "train loss:0.47802378106720994\n",
      "train loss:0.6052788444801787\n",
      "train loss:0.45293480358098587\n",
      "train loss:0.5216489216487981\n",
      "train loss:0.49051426998467845\n",
      "train loss:0.4598900913383032\n",
      "train loss:0.6325089227506898\n",
      "train loss:0.5080743434262939\n",
      "train loss:0.7110313852492187\n",
      "train loss:0.6036248363919763\n",
      "train loss:0.5108482445959257\n",
      "train loss:0.5309369224209487\n",
      "train loss:0.47106216492629516\n",
      "train loss:0.4024767323459075\n",
      "train loss:0.4850088281145002\n",
      "train loss:0.5869117322507428\n",
      "train loss:0.5171646225754603\n",
      "train loss:0.5859244374842055\n",
      "train loss:0.6050485830541064\n",
      "train loss:0.3245931118262366\n",
      "train loss:0.5164368142507599\n",
      "train loss:0.6474833393889399\n",
      "train loss:0.7019285778685814\n",
      "train loss:0.4556953545985475\n",
      "train loss:0.28679839749545\n",
      "train loss:0.44169756337111077\n",
      "train loss:0.4236465636027518\n",
      "train loss:0.43570277240673183\n",
      "train loss:0.46491099444794565\n",
      "train loss:0.2881452526008924\n",
      "train loss:0.4737631377975849\n",
      "train loss:0.3348217100451611\n",
      "train loss:0.68263589559194\n",
      "train loss:0.4403157151753556\n",
      "train loss:0.405686725588273\n",
      "train loss:0.3902919676452558\n",
      "train loss:0.41765547577357104\n",
      "train loss:0.35876865861168994\n",
      "train loss:0.2936789638861648\n",
      "train loss:0.39708621189909055\n",
      "train loss:0.33887760535275036\n",
      "train loss:0.4031953599965254\n",
      "train loss:0.30708070085778344\n",
      "train loss:0.43774241350528004\n",
      "train loss:0.2756058967266775\n",
      "train loss:0.27767019818870564\n",
      "train loss:0.5817087071307245\n",
      "train loss:0.581220329210616\n",
      "train loss:0.3449105971644212\n",
      "train loss:0.3842125654279354\n",
      "train loss:0.29994396488546193\n",
      "train loss:0.314465482055286\n",
      "train loss:0.3284293856568216\n",
      "train loss:0.368249215044281\n",
      "train loss:0.32219677540242364\n",
      "train loss:0.42251220147117485\n",
      "train loss:0.2598783825986516\n",
      "train loss:0.37790488104415715\n",
      "train loss:0.19267348552432917\n",
      "train loss:0.3411799763845454\n",
      "train loss:0.36044345602437294\n",
      "train loss:0.22954319846551244\n",
      "train loss:0.30635454221353237\n",
      "train loss:0.32609852792683375\n",
      "train loss:0.2625404003655845\n",
      "train loss:0.25275219326286463\n",
      "train loss:0.31197429822837036\n",
      "train loss:0.44797169079285054\n",
      "train loss:0.29849881816333884\n",
      "train loss:0.2915242797249468\n",
      "train loss:0.38271580758429413\n",
      "train loss:0.3353842401964062\n",
      "train loss:0.33875164479245645\n",
      "train loss:0.22841234742012184\n",
      "train loss:0.3724711783642537\n",
      "train loss:0.4676326761605499\n",
      "train loss:0.4441352277229357\n",
      "train loss:0.4550050390049064\n",
      "train loss:0.4262415301172465\n",
      "train loss:0.18936677475955516\n",
      "train loss:0.34795962596579955\n",
      "train loss:0.3340488266776596\n",
      "train loss:0.23350938369404023\n",
      "train loss:0.30734776469644176\n",
      "train loss:0.27412985898412656\n",
      "train loss:0.33269634000244147\n",
      "train loss:0.3107215275812002\n",
      "train loss:0.317064693184027\n",
      "train loss:0.27959655382331466\n",
      "train loss:0.5220051541068584\n",
      "train loss:0.2884420329731855\n",
      "train loss:0.3906053969632267\n",
      "train loss:0.39244978425557014\n",
      "train loss:0.2957804518437159\n",
      "train loss:0.4051449504241384\n",
      "train loss:0.2740647058804042\n",
      "train loss:0.4323656940226553\n",
      "train loss:0.22707239467164136\n",
      "train loss:0.23297850940676554\n",
      "train loss:0.28248640351378534\n",
      "train loss:0.35798916798844216\n",
      "train loss:0.37730224928382117\n",
      "train loss:0.18586182825351358\n",
      "train loss:0.3364476287191479\n",
      "train loss:0.20695277285845542\n",
      "train loss:0.1856817078879782\n",
      "train loss:0.32636384964715615\n",
      "train loss:0.2734299220093867\n",
      "train loss:0.3088469111986211\n",
      "train loss:0.3261733275009769\n",
      "train loss:0.3891559574173524\n",
      "train loss:0.39483603724738614\n",
      "train loss:0.2072998390577065\n",
      "train loss:0.268908591016011\n",
      "train loss:0.20247551921328608\n",
      "train loss:0.1371793694579061\n",
      "train loss:0.3203643301441138\n",
      "train loss:0.19107393828573316\n",
      "train loss:0.33852832897014445\n",
      "train loss:0.17080165580879605\n",
      "train loss:0.38511799021715537\n",
      "train loss:0.32499251403216667\n",
      "train loss:0.2791596904449838\n",
      "train loss:0.22679690003462277\n",
      "train loss:0.12313537204913061\n",
      "train loss:0.18729884362163773\n",
      "train loss:0.24266497304165607\n",
      "train loss:0.33061670954695765\n",
      "train loss:0.27640983238780165\n",
      "train loss:0.40695808929385185\n",
      "train loss:0.2661368745672766\n",
      "train loss:0.256086140726287\n",
      "train loss:0.22726866717776167\n",
      "train loss:0.18453641607639226\n",
      "train loss:0.3042560278316989\n",
      "train loss:0.2325852041267636\n",
      "train loss:0.10391377778733023\n",
      "train loss:0.3813554783009536\n",
      "train loss:0.18431352705563456\n",
      "train loss:0.160588239054938\n",
      "train loss:0.18224514857825796\n",
      "train loss:0.28648989316660517\n",
      "train loss:0.33986071545468766\n",
      "train loss:0.2259718645063313\n",
      "train loss:0.16488436616348934\n",
      "train loss:0.2165688848586987\n",
      "train loss:0.22835860537670002\n",
      "train loss:0.20794788325065716\n",
      "train loss:0.3645391375219635\n",
      "train loss:0.09483062709863058\n",
      "train loss:0.16699358998454092\n",
      "train loss:0.2839220332025476\n",
      "train loss:0.12664887068150946\n",
      "train loss:0.25642924514114\n",
      "train loss:0.2514566523490013\n",
      "train loss:0.18787253977727883\n",
      "train loss:0.28984124340428463\n",
      "train loss:0.42536077526432886\n",
      "train loss:0.18280463824988832\n",
      "train loss:0.3771422043099663\n",
      "train loss:0.28723266935141006\n",
      "train loss:0.38656830425889255\n",
      "train loss:0.27730580223232343\n",
      "train loss:0.15956148422331065\n",
      "train loss:0.27196077310527667\n",
      "train loss:0.14903892512258213\n",
      "train loss:0.10978328656883635\n",
      "train loss:0.19921286293658708\n",
      "train loss:0.1699505222023183\n",
      "train loss:0.1801529476116518\n",
      "train loss:0.225571019255417\n",
      "train loss:0.32900495760969845\n",
      "train loss:0.31810397263377044\n",
      "train loss:0.25471348449051606\n",
      "train loss:0.2052446871879892\n",
      "train loss:0.1260169520762603\n",
      "train loss:0.2402050103703421\n",
      "train loss:0.17622653356019724\n",
      "train loss:0.13192024089338739\n",
      "train loss:0.20614751764458578\n",
      "train loss:0.06082362903060345\n",
      "train loss:0.264056122781524\n",
      "train loss:0.28190301777073223\n",
      "train loss:0.22349750291375714\n",
      "train loss:0.26009282732658556\n",
      "train loss:0.2575855904600546\n",
      "train loss:0.11676572718561626\n",
      "train loss:0.2338568603951459\n",
      "train loss:0.10275357804356343\n",
      "train loss:0.17530163356943515\n",
      "train loss:0.1488385418268566\n",
      "train loss:0.18497785987512605\n",
      "train loss:0.2671695440010845\n",
      "train loss:0.29984688906875556\n",
      "train loss:0.17439198704576275\n",
      "train loss:0.22655699480793634\n",
      "train loss:0.2711722260008024\n",
      "train loss:0.20753182442940596\n",
      "train loss:0.17967960765373645\n",
      "train loss:0.2290543217099624\n",
      "train loss:0.15562454066912323\n",
      "train loss:0.24021487930802027\n",
      "train loss:0.1525534063726035\n",
      "train loss:0.18902393640982687\n",
      "train loss:0.2743776700716587\n",
      "train loss:0.1500460901387605\n",
      "train loss:0.33581329646058977\n",
      "train loss:0.3242519523760466\n",
      "train loss:0.09991331989231089\n",
      "train loss:0.10375132773980521\n",
      "train loss:0.22000215978905238\n",
      "train loss:0.0757976990534346\n",
      "train loss:0.22718697428840978\n",
      "train loss:0.19458865225015337\n",
      "train loss:0.1594760568640831\n",
      "train loss:0.16433465825715693\n",
      "train loss:0.18502363272155498\n",
      "train loss:0.2673048274081966\n",
      "train loss:0.20741348727296063\n",
      "train loss:0.18411885005125622\n",
      "train loss:0.14025111551020264\n",
      "train loss:0.18931316915043786\n",
      "train loss:0.2770050173785284\n",
      "train loss:0.152535976215841\n",
      "train loss:0.15924762059098888\n",
      "train loss:0.1175976097766146\n",
      "train loss:0.30159581573888683\n",
      "train loss:0.3002880855465802\n",
      "train loss:0.3117000362236447\n",
      "train loss:0.2146609434898175\n",
      "train loss:0.26211320692518614\n",
      "train loss:0.2745374157893926\n",
      "train loss:0.1700713320411501\n",
      "train loss:0.17386044767934042\n",
      "train loss:0.17426805471936507\n",
      "train loss:0.23930849182636152\n",
      "train loss:0.11649693018375887\n",
      "train loss:0.20271723463002822\n",
      "train loss:0.15964041241396873\n",
      "train loss:0.14059802778861788\n",
      "train loss:0.18297294992529356\n",
      "train loss:0.2376525937690801\n",
      "train loss:0.14757405975849386\n",
      "train loss:0.22788164388198656\n",
      "train loss:0.342383499001847\n",
      "train loss:0.10826676078066823\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 20\u001b[0m\n\u001b[0;32m     12\u001b[0m network \u001b[39m=\u001b[39m SimpleConvNet(input_dim\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m28\u001b[39m,\u001b[39m28\u001b[39m), \n\u001b[0;32m     13\u001b[0m                         conv_param \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mfilter_num\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m30\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfilter_size\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpad\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstride\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m},\n\u001b[0;32m     14\u001b[0m                         hidden_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, output_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, weight_init_std\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m     16\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001b[0;32m     17\u001b[0m                   epochs\u001b[39m=\u001b[39mmax_epochs, mini_batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[0;32m     18\u001b[0m                   optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m'\u001b[39m, optimizer_param\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.001\u001b[39m},\n\u001b[0;32m     19\u001b[0m                   evaluate_sample_num_per_epoch\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[19], line 68\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     67\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter):\n\u001b[1;32m---> 68\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step()\n\u001b[0;32m     70\u001b[0m     test_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39maccuracy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_test, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_test)\n\u001b[0;32m     72\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "Cell \u001b[1;32mIn[19], line 44\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mgradient(x_batch, t_batch)\n\u001b[0;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparams, grads)\n\u001b[1;32m---> 44\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork\u001b[39m.\u001b[39;49mloss(x_batch, t_batch)\n\u001b[0;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loss_list\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtrain loss:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(loss))\n",
      "File \u001b[1;32mc:\\Users\\HCEUN\\github\\221220_DeepLearing_from_scratch\\ch7_CNN\\common\\simple_convnet.py:75\u001b[0m, in \u001b[0;36mSimpleConvNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, x, t):\n\u001b[0;32m     68\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"손실 함수를 구한다.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39m    t : 정답 레이블\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x)\n\u001b[0;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_layer\u001b[39m.\u001b[39mforward(y, t)\n",
      "File \u001b[1;32mc:\\Users\\HCEUN\\github\\221220_DeepLearing_from_scratch\\ch7_CNN\\common\\simple_convnet.py:63\u001b[0m, in \u001b[0;36mSimpleConvNet.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     62\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m---> 63\u001b[0m         x \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(x)\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\HCEUN\\github\\221220_DeepLearing_from_scratch\\ch7_CNN\\common\\layers.py:223\u001b[0m, in \u001b[0;36mConvolution.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    220\u001b[0m col \u001b[39m=\u001b[39m im2col(x, FH, FW, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad)\n\u001b[0;32m    221\u001b[0m col_W \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW\u001b[39m.\u001b[39mreshape(FN, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mT\n\u001b[1;32m--> 223\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(col, col_W) \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb\n\u001b[0;32m    224\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mreshape(N, out_h, out_w, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from common.simple_convnet import SimpleConvNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3adb6de1",
   "metadata": {},
   "source": [
    "* 책코드와 깃허브 코드가 다름 (self.conv) + backward 없음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8c0ffc6b8f05b006c73f2cd680928154ddfcde68c31c80ce450bbea892f4c22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

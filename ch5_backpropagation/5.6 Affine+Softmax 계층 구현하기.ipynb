{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99b825d0",
   "metadata": {},
   "source": [
    "# 5.6.1 Affine 계층\n",
    "\n",
    "np.dot으로 행렬의 곱셈을 표현했던 것을 기억해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81387d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) -> 1 x 2\n",
      "(2, 3) -> 2 x 3\n",
      "(3,) -> 1 x 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.42270627, 0.78388733, 1.25228132])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(2)    # input\n",
    "W = np.random.rand(2, 3) # weight\n",
    "B = np.random.rand(3)    # bias\n",
    "\n",
    "print(X.shape, '-> 1 x 2')\n",
    "print(W.shape, '-> 2 x 3')\n",
    "print(B.shape, '-> 1 x 3')\n",
    "\n",
    "np.dot(X, W) + B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c121575",
   "metadata": {},
   "source": [
    "행렬의 곱은 아래와 같은 특징이 있다.\n",
    "\n",
    "* 행렬곱을 하기 위해서는 앞/뒤의 차원 수가 같아야 한다.\n",
    "* 행렬의 곱을 기하학에서는 Affine 변환이라고 한다.\n",
    "\n",
    "이 Affine 변환 역시 계산 그래프로 표현 가능하다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a70f68d",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\"><img src=\"imgs/5-24.png\" width=600></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b79c1d33",
   "metadata": {},
   "source": [
    "왜 이제와서 affine이 나오는가? 지금까지의 계산은 스칼라값이 흘렀는데, 이제 행렬이 흐르고 있기 때문이다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d86dcaf",
   "metadata": {},
   "source": [
    "* 아래 예시에서는 편의상 backward에 x를 그대로 넣었다\n",
    "* 하지만 실제 계산에서는, global loss로부터 계산된 배열이 들어간다.\n",
    "* 그러니 값들은 그냥 참고용으로만 알아두자.\n",
    "\n",
    "좋은 자료: 오차 역전파를 계산그래프로 설명하는 유튜브 영상\n",
    "https://www.youtube.com/watch?v=1Q_etC_GHHk&t=606s\n",
    "\n",
    "# 5.6.2 배치용 Affine 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9861246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_dot_W: \n",
      " [[ 0  0  0]\n",
      " [10 10 10]]\n",
      "B: [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])\n",
    "\n",
    "print('X_dot_W: \\n', X_dot_W)\n",
    "print('B:', B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc0c46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W + B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd23d8b1",
   "metadata": {},
   "source": [
    "위와 같은 현상이 일어나는 것은 numpy가 가지고 있는 broadcasting 기능 덕분이다. 이는 shape가 작은 행렬이 더 큰 행렬에 맞춰서 알아서 크기가 조절되는 것을 말한다. (확장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3688a6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49f9ec45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY, axis = 0)\n",
    "dB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7977392d",
   "metadata": {},
   "source": [
    "Affine 계층의 bias 계산되는 원리를 이해하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e256f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08d759bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "B = np.array([4, 4, 4])\n",
    "a = Affine(W, B)\n",
    "\n",
    "X = np.array([1.0, 2.0])\n",
    "a.forward(X)\n",
    "print(a.original_x_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59fd5977",
   "metadata": {},
   "source": [
    "# 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "소프트맥스 함수는 입력 값을 정규화하여 출력한다. 예를 들어 손글씨 숫자 인식에서의 softmax 계층 출력은 아래 그림 처럼 된다. 3번째 확률값이 제일 큰 것에 주목!\n",
    "\n",
    "<p align=\"center\"><img src=\"imgs/5-28.png\" width=600></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9986c7b0",
   "metadata": {},
   "source": [
    "이제 softmax와 cross-entropy를 합친 softmax-with-loss라는 클래스를 구현해볼 것이다. 2개를 합친 것을 그림으로 나타내면 아래와 같다.\n",
    "\n",
    "<p align=\"center\"><img src=\"imgs/5-29.png\" width=600></p>\n",
    "\n",
    "뭐가 굉장히 복잡해 보이는데, 사실 softmax와 cross-entropy를 붙여놓은 것이라 크게 어렵지는 않다. 이걸 간소화 하면 밑에 처럼 표현할 수 있다.\n",
    "\n",
    "<p align=\"center\"><img src=\"imgs/5-30.png\" width=600></p>\n",
    "\n",
    "여기서 y는 softmax의 출력값이고, t는 원래 라벨이다. 앞에서 설명한건 시그모이드고, 이 책의 어디에서도 softmax와 cross entropy의 계산그래프에 대해 설명하지 않았다. 그러니 아래 포인트 정도만 이해하고 다음으로 넘어가면 된다. 궁금한 사람들은 각자 부록을 공부하면 된다!\n",
    "\n",
    "* input과 output에만 주목하자. 결국 미분의 output은 y-t가 되는데, 원래 라벨에 대한 정보가 담겨 있다.\n",
    "* 라벨과 예측한 값에 대한 오차가 크면? -> 많이 움직여야 한다.\n",
    "* 역전파의 결과가 상당히 깔끔한데? -> 원래 그렇게 설계되었기 때문이다.\n",
    "\n",
    "이를 코드로 구현해보면 아래와 같이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eec83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import layers\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # 소프트맥스 출력\n",
    "        self.x = None # 정답 라벨\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = layers.softmax(x)\n",
    "        self.loss = layers.cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size # 배치사이즈로 나눠주는 이유? -> 이미지 1개당 오차를 전달해야 하니까\n",
    "        return dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "429d3c4ab745cb5040d1082d56ce2bddcd2ae65139d012657fade8b90ce91472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

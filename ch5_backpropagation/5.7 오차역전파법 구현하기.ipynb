{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99b825d0",
   "metadata": {},
   "source": [
    "# 5.7.1 신경망 학습의 전체 그림\n",
    "\n",
    "신경망 학습은 아래의 순서로 반복된다.\n",
    "\n",
    "1. 미니배치 - 훈련 데이터 중 일부를 가져옴\n",
    "2. 기울기 산출 - 가중치 매개변수의 기울기를 구함\n",
    "3. 매개변수 갱신 - 가충치 매개변수를 기울기 방향으로 조금 갱신\n",
    "\n",
    "위 3단계를 계속 반복한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a70f68d",
   "metadata": {},
   "source": [
    "# 5.7.2 오차 역전파법을 이용한 신경망 구현하기\n",
    "\n",
    "* OrderedDict 용법 알기\n",
    "* Affine 계층 용법 알기\n",
    "* 역전파 때 미분값의 최초값은 1이다. (진짜 1이었다니...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9688da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # ★★★★ 1 ★★★★ \n",
    "        # 순서가 있는 딕셔너리\n",
    "        self.layers = OrderedDict() \n",
    "\n",
    "        # ★★★★ 2 ★★★★\n",
    "        # Affine은 x * W + b 해주는 역할\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1']) \n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1 # ★★★★ 3 ★★★★ \n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8516671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<common.layers.Affine object at 0x000001BD66AF9BB0>\n",
      "<common.layers.Relu object at 0x000001BD16172C40>\n",
      "<common.layers.Affine object at 0x000001BD161729A0>\n",
      "----\n",
      "<common.layers.Affine object at 0x000001BD161729A0>\n",
      "<common.layers.Relu object at 0x000001BD16172C40>\n",
      "<common.layers.Affine object at 0x000001BD66AF9BB0>\n"
     ]
    }
   ],
   "source": [
    "two = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in reversed(two.layers.values()):\n",
    "    print(i)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for j in two.layers.values():\n",
    "    print(j)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e311261",
   "metadata": {},
   "source": [
    "# 5.7.3 오차 역전파법으로 기울기 검증하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3568263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "W1:0.0006751023017552469\n",
      "b1:0.0035592502616701538\n",
      "W2:0.006811786361545174\n",
      "b2:0.00030747053899727597\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "import time\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "# 실제로 학습할 때는 아래 두 줄 중 하나만 써야 함.\n",
    "\n",
    "stime = time.time()\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "print(time.time() - stime)\n",
    "\n",
    "stime = time.time()\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "print(time.time() - stime)\n",
    "\n",
    "# 오차 평균\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cfdcd7a",
   "metadata": {},
   "source": [
    "가중치의 오차가 매우 작다. 그러니 여기서의 포인트는 수치미분으로 구한 기울기함수와 우리가 오차 역전파로 계산한 기울기 값이 거의 동일하다는 것을 보여준다. 그럼 오차역전파를 굳이 쓰는 이유는? 우선 빠르다. time.time을 넣어서 비교해보면 어마어마하게 차이가 난다.\n",
    "\n",
    "* 수치미분은 계산이 느리다.\n",
    "* 우리가 이번 장에서 구현한 오차역전파는 행렬로 계산을 한꺼번에 한다!\n",
    "\n",
    "# 5.7.4 학습 구현하기\n",
    "\n",
    "단순히 수치미분을 오차역전파로 바꿔준 것이다. 각자 구현해보면 좋다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3c42619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11875 0.1232\n",
      "0.9020833333333333 0.908\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 31\u001b[0m\n\u001b[0;32m     27\u001b[0m t_batch \u001b[39m=\u001b[39m t_train[batch_mask]\n\u001b[0;32m     29\u001b[0m \u001b[39m# 기울기 계산\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m grad \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mgradient(x_batch, t_batch) \u001b[39m# 오차역전파법 방식 ★★★★★\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m# 갱신\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mW1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mW2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mb2\u001b[39m\u001b[39m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn [5], line 65\u001b[0m, in \u001b[0;36mTwoLayerNet.gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient\u001b[39m(\u001b[39mself\u001b[39m, x, t):\n\u001b[0;32m     64\u001b[0m     \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(x, t)\n\u001b[0;32m     67\u001b[0m     \u001b[39m# backward\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     dout \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# ★★★★ 3 ★★★★ \u001b[39;00m\n",
      "Cell \u001b[1;32mIn [5], line 40\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, x, t):\n\u001b[1;32m---> 40\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x)\n\u001b[0;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastLayer\u001b[39m.\u001b[39mforward(y, t)\n",
      "Cell \u001b[1;32mIn [5], line 34\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     33\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m---> 34\u001b[0m         x \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(x)\n\u001b[0;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\white\\github\\221220_DeepLearing_from_scratch\\ch5_backpropagation\\common\\layers.py:57\u001b[0m, in \u001b[0;36mAffine.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m x\n\u001b[1;32m---> 57\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n\u001b[0;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식 ★★★★★\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "429d3c4ab745cb5040d1082d56ce2bddcd2ae65139d012657fade8b90ce91472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

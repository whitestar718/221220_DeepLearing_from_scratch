{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99b825d0",
   "metadata": {},
   "source": [
    "# 5.7.1 신경망 학습의 전체 그림\n",
    "\n",
    "신경망 학습은 아래의 순서로 반복된다.\n",
    "\n",
    "1. 미니배치 - 훈련 데이터 중 일부를 가져옴\n",
    "2. 기울기 산출 - 가중치 매개변수의 기울기를 구함\n",
    "3. 매개변수 갱신 - 가충치 매개변수를 기울기 방향으로 조금 갱신\n",
    "\n",
    "위 3단계를 계속 반복한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a70f68d",
   "metadata": {},
   "source": [
    "# 5.7.2 오차 역전파법을 이용한 신경망 구현하기\n",
    "\n",
    "* OrderedDict 용법 알기\n",
    "* Affine 계층 용법 알기\n",
    "* 역전파 때 미분값의 최초값은 1이다. (진짜 1이었다니...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9688da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # ★★★★ 1 ★★★★ \n",
    "        # 순서가 있는 딕셔너리\n",
    "        self.layers = OrderedDict() \n",
    "\n",
    "        # ★★★★ 2 ★★★★\n",
    "        # Affine은 x * W + b 해주는 역할\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1']) \n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1 # ★★★★ 3 ★★★★ \n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e311261",
   "metadata": {},
   "source": [
    "# 5.7.3 오차 역전파법으로 기울기 검증하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3568263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.008886337280273\n",
      "0.0005271434783935547\n",
      "W1:3.863266630601832e-10\n",
      "b1:2.0648986970490424e-09\n",
      "W2:5.482554415346625e-09\n",
      "b2:1.401652278423815e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "import time\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "# 실제로 학습할 때는 아래 두 줄 중 하나만 써야 함.\n",
    "\n",
    "stime = time.time()\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "print(time.time() - stime)\n",
    "\n",
    "stime = time.time()\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "print(time.time() - stime)\n",
    "\n",
    "# 오차 평균\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cfdcd7a",
   "metadata": {},
   "source": [
    "가중치의 오차가 매우 작다. 그러니 여기서의 포인트는 수치미분으로 구한 기울기함수와 우리가 오차 역전파로 계산한 기울기 값이 거의 동일하다는 것을 보여준다. 그럼 오차역전파를 굳이 쓰는 이유는? 우선 빠르다. time.time을 넣어서 비교해보면 어마어마하게 차이가 난다.\n",
    "\n",
    "* 수치미분은 계산이 느리다.\n",
    "* 우리가 이번 장에서 구현한 오차역전파는 행렬로 계산을 한꺼번에 한다!\n",
    "\n",
    "# 5.7.4 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c42619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식 ★★★★★\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "429d3c4ab745cb5040d1082d56ce2bddcd2ae65139d012657fade8b90ce91472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

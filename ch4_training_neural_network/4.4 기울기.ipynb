{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b295143c",
   "metadata": {},
   "source": [
    "앞에서는 $x_0$과 $x_1$의 편미분을 변수별로 따로 계산했다. 그런데 이를 동시에 계산하고 싶다면 어떻게 해야 할까? 가령 $x_0=3$, $x_1=4$ 일 때, $(x_0, x_1)$ 양쪽의 편미분을 묶어서 $({\\partial{f} \\over x_0},{\\partial{f} \\over x_1})$을 계산해보자. 이 때, $({\\partial{f} \\over x_0},{\\partial{f} \\over x_1})$ 처럼 모든 변수의 편미분을 벡터로 정리한 것을 $기울기^{gradient}$라고 한다. 기울기는 예를 들어 다음과 같이 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09c7d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        # f(x+h) 계산\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / 2*h\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "\n",
    "    # print('grad:', grad)\n",
    "\n",
    "    return grad\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9aecaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad: [6.e-08 8.e-08]\n",
      "[6.e-08 8.e-08]\n",
      "grad: [0.e+00 4.e-08]\n",
      "[0.e+00 4.e-08]\n",
      "grad: [6.e-08 0.e+00]\n",
      "[6.e-08 0.e+00]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7cd61ab",
   "metadata": {},
   "source": [
    "각각의 기울기를 정리하면 아래와 같다. <br>\n",
    "* (3, 4) -> grad: (6.0, 8.0)\n",
    "* (0, 2) -> grad: (0.0, 4.0)\n",
    "* (3, 0) -> grad: (6.0, 0.0)\n",
    "\n",
    "이러한 기울기들은 무엇을 의미하는 걸까? 그림으로 그려보면 이해가 될 것이다. 다만, 여기에서는 기울기의 결과에 마이너스를 붙인 백터를 그려보겠다. 기울기 그림은 아래처럼 방향을 가진 벡터(화살표)로 그려진다. 이 그림을 보면 기울기는 함수의 가장 낮은 장소를 가리키는 것 같다. 마치 나침반처럼 화살표들은 한 점을 향하고 있다. '가장 낮은 곳'에서 멀어질수록 화살표의 크기도 커짐을 알 수 있다.\n",
    "\n",
    "<p align=\"center\"><img src=\"imgs/4-9.png\" width=500></p>\n",
    "\n",
    "위의 그림에서 기울기는 가장 낮은 장소를 가리키지만, 실제로는 반드시 그렇다고도 할 수 없다. 사실 기울기는 각 지점에서 낮아지는 방향을 의미한다. 더 정확히 말하면, 기울기가 가리키는 쪽은 각 장소에서 함수의 출력값을 가장 크게 줄이는 방향인 것이다.\n",
    "\n",
    "# 4.4.1 경사법(경사하강법)\n",
    "\n",
    "기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아낸다. 신경망 역시 최적의 매개변수를 학습 시에 찾아야 한다. 여기서 최적이란, 손실함수가 최소값이 될 때를 의미한다. 그러나 일반적인 문제의 손실함수는 매우 복잡해서 어디가 최솟점인지 찾기가 어렵다. 이럴 때 기울기를 이용하여, 함수의 최소 지점을 찾으려는 것이 경사 하강법이다. 여기서 주의할 점은 각 지점에서 함수의 값을 낮추는 지표가 기울기라는 것이다. 그러나 기울기가 가리키는 곳에 정말 함수의 최소값이 있는지는 보장할 수 없다. 실제로 복잡한 함수에서는 기울기가 가리키는 곳에 최솟값이 없는 경우가 대부분이다.\n",
    "<br>\n",
    "기울어진 방향이 꼭 최소값을 가리키는 것은 아니나, 그 방향으로 가야 함수의 값을 줄일 수 있다. 그래서 사실 선택의 여지가 별로 없긴 하다. (이 방법에서는!) 드디어 경사하강법을 소개한다. 경사하강법은 현 위치에서 기울어진 방향만큼 이동하는 것이다. 경사하강법을 수식으로 나타내면 아래와 같다.\n",
    "\n",
    "$$\\huge{x_0 = x_0 - \\eta{\\partial f \\over \\partial x_0}}$$\n",
    "$$\\huge{x_1 = x_1 - \\eta{\\partial f \\over \\partial x_1}}$$\n",
    "\n",
    "위의 식에서 $\\eta^{에타}$는 생신하는 양을 나타낸다. 이를 learning rate라고 한다. 한 번의 학습으로 얼마만큼을 학습해야 할 지, 즉 매개변수의 값을 얼마나 갱신하는 지를 정하는 것이 학습률이다. 위의 식은 1회에 해당하는 것이고 이를 여러 번 반복하면서 서서히 함수의 값을 줄여나가게 된다. 또 여기에서는 변수가 2개인 경우만 살펴봤지만, 변수가 늘어도 같은 식으로 갱신이 된다. 또한 학습률은 0.01이나 0.001 등 미리 특정 값으로 정해둬야 하는데, 일반적으로 너무 크거나 너무 작으면 '좋은 장소'를 찾아갈 수 없다. 경사 하강법은 다음과 같이 간단하게 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa83a9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        print('grad:', grad)\n",
    "        x -= (lr * grad)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abe97145",
   "metadata": {},
   "source": [
    "문제. 경사법으로 $f(x_0, x_1) = x_0^2+x_1^2$의 최솟값을 구하여라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "662b232a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.e-08  8.e-08]\n",
      "[-5.99999999e-08  7.99999998e-08]\n",
      "[-5.99999998e-08  7.99999997e-08]\n",
      "[-5.99999996e-08  7.99999995e-08]\n",
      "[-5.99999995e-08  7.99999994e-08]\n",
      "[-5.99999994e-08  7.99999992e-08]\n",
      "[-5.99999993e-08  7.99999990e-08]\n",
      "[-5.99999992e-08  7.99999989e-08]\n",
      "[-5.99999990e-08  7.99999987e-08]\n",
      "[-5.99999989e-08  7.99999986e-08]\n",
      "[-5.99999988e-08  7.99999984e-08]\n",
      "[-5.99999987e-08  7.99999982e-08]\n",
      "[-5.99999986e-08  7.99999981e-08]\n",
      "[-5.99999984e-08  7.99999979e-08]\n",
      "[-5.99999983e-08  7.99999978e-08]\n",
      "[-5.99999982e-08  7.99999976e-08]\n",
      "[-5.99999981e-08  7.99999974e-08]\n",
      "[-5.99999980e-08  7.99999973e-08]\n",
      "[-5.99999978e-08  7.99999971e-08]\n",
      "[-5.99999977e-08  7.99999970e-08]\n",
      "[-5.99999976e-08  7.99999968e-08]\n",
      "[-5.99999975e-08  7.99999966e-08]\n",
      "[-5.99999974e-08  7.99999965e-08]\n",
      "[-5.99999972e-08  7.99999963e-08]\n",
      "[-5.99999971e-08  7.99999962e-08]\n",
      "[-5.9999997e-08  7.9999996e-08]\n",
      "[-5.99999969e-08  7.99999958e-08]\n",
      "[-5.99999968e-08  7.99999957e-08]\n",
      "[-5.99999966e-08  7.99999955e-08]\n",
      "[-5.99999965e-08  7.99999954e-08]\n",
      "[-5.99999964e-08  7.99999952e-08]\n",
      "[-5.99999963e-08  7.99999950e-08]\n",
      "[-5.99999962e-08  7.99999949e-08]\n",
      "[-5.99999960e-08  7.99999947e-08]\n",
      "[-5.99999959e-08  7.99999946e-08]\n",
      "[-5.99999958e-08  7.99999944e-08]\n",
      "[-5.99999957e-08  7.99999942e-08]\n",
      "[-5.99999956e-08  7.99999941e-08]\n",
      "[-5.99999954e-08  7.99999939e-08]\n",
      "[-5.99999953e-08  7.99999938e-08]\n",
      "[-5.99999952e-08  7.99999936e-08]\n",
      "[-5.99999951e-08  7.99999934e-08]\n",
      "[-5.99999950e-08  7.99999933e-08]\n",
      "[-5.99999948e-08  7.99999931e-08]\n",
      "[-5.99999947e-08  7.99999930e-08]\n",
      "[-5.99999946e-08  7.99999928e-08]\n",
      "[-5.99999945e-08  7.99999926e-08]\n",
      "[-5.99999944e-08  7.99999925e-08]\n",
      "[-5.99999942e-08  7.99999923e-08]\n",
      "[-5.99999941e-08  7.99999922e-08]\n",
      "[-5.9999994e-08  7.9999992e-08]\n",
      "[-5.99999939e-08  7.99999918e-08]\n",
      "[-5.99999938e-08  7.99999917e-08]\n",
      "[-5.99999936e-08  7.99999915e-08]\n",
      "[-5.99999935e-08  7.99999914e-08]\n",
      "[-5.99999934e-08  7.99999912e-08]\n",
      "[-5.99999933e-08  7.99999910e-08]\n",
      "[-5.99999932e-08  7.99999909e-08]\n",
      "[-5.99999930e-08  7.99999907e-08]\n",
      "[-5.99999929e-08  7.99999906e-08]\n",
      "[-5.99999928e-08  7.99999904e-08]\n",
      "[-5.99999927e-08  7.99999902e-08]\n",
      "[-5.99999926e-08  7.99999901e-08]\n",
      "[-5.99999924e-08  7.99999899e-08]\n",
      "[-5.99999923e-08  7.99999898e-08]\n",
      "[-5.99999922e-08  7.99999896e-08]\n",
      "[-5.99999921e-08  7.99999894e-08]\n",
      "[-5.99999920e-08  7.99999893e-08]\n",
      "[-5.99999918e-08  7.99999891e-08]\n",
      "[-5.99999917e-08  7.99999890e-08]\n",
      "[-5.99999916e-08  7.99999888e-08]\n",
      "[-5.99999915e-08  7.99999886e-08]\n",
      "[-5.99999914e-08  7.99999885e-08]\n",
      "[-5.99999912e-08  7.99999883e-08]\n",
      "[-5.99999911e-08  7.99999882e-08]\n",
      "[-5.9999991e-08  7.9999988e-08]\n",
      "[-5.99999909e-08  7.99999878e-08]\n",
      "[-5.99999908e-08  7.99999877e-08]\n",
      "[-5.99999906e-08  7.99999875e-08]\n",
      "[-5.99999905e-08  7.99999874e-08]\n",
      "[-5.99999904e-08  7.99999872e-08]\n",
      "[-5.99999903e-08  7.99999870e-08]\n",
      "[-5.99999902e-08  7.99999869e-08]\n",
      "[-5.99999900e-08  7.99999867e-08]\n",
      "[-5.99999899e-08  7.99999866e-08]\n",
      "[-5.99999898e-08  7.99999864e-08]\n",
      "[-5.99999897e-08  7.99999862e-08]\n",
      "[-5.99999896e-08  7.99999861e-08]\n",
      "[-5.99999894e-08  7.99999859e-08]\n",
      "[-5.99999893e-08  7.99999858e-08]\n",
      "[-5.99999892e-08  7.99999856e-08]\n",
      "[-5.99999891e-08  7.99999854e-08]\n",
      "[-5.99999890e-08  7.99999853e-08]\n",
      "[-5.99999888e-08  7.99999851e-08]\n",
      "[-5.99999887e-08  7.99999850e-08]\n",
      "[-5.99999886e-08  7.99999848e-08]\n",
      "[-5.99999885e-08  7.99999846e-08]\n",
      "[-5.99999884e-08  7.99999845e-08]\n",
      "[-5.99999882e-08  7.99999843e-08]\n",
      "[-5.99999881e-08  7.99999842e-08]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.9999994,  3.9999992])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x, 0.1, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "429d3c4ab745cb5040d1082d56ce2bddcd2ae65139d012657fade8b90ce91472"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
